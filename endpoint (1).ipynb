!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7
!pip install huggingface_hub
import torch
from trl import SFTTrainer
from peft import LoraConfig
from datasets import load_dataset
from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline)

model = AutoModelForCausalLM.from_pretrained(
                                              pretrained_model_name_or_path="aboonaji/llama2finetune-v2",
                                              quantization_config=BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=getattr(torch, "float16"), bnb_4bit_quant_type="nf4")
                                            )
model.config.use_cache = False
model.config.pretraining_tp = 1

tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path = "aboonaji/llama2finetune-v2", trust_remote_code = True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

training_arguments = TrainingArguments(
    output_dir = "./results",
    per_device_train_batch_size = 4,
    # per_device_eval_batch_size=2,
    max_steps = 100
    )

trainer = SFTTrainer(
    model=model,
    args = training_arguments,
    tokenizer = tokenizer,
    train_dataset = load_dataset(path="vivekdugale/falcon7B_filtered_dataset_amod_helios_450", split='train'),
    peft_config = LoraConfig(task_type="CAUSAL_LM", r=64, lora_alpha=16, lora_dropout=0.1),
    dataset_text_field = "text"
    )

trainer.train()

text_gen_pipeline = pipeline(task = "text-generation", model = model, tokenizer = tokenizer, max_length = 100)
