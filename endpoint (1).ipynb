!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7
!pip install huggingface_hub
import torch
from trl import SFTTrainer
from peft import LoraConfig
from datasets import load_dataset
from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline)

model = AutoModelForCausalLM.from_pretrained(
                                              pretrained_model_name_or_path="aboonaji/llama2finetune-v2",
                                              quantization_config=BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=getattr(torch, "float16"), bnb_4bit_quant_type="nf4")
                                            )
model.config.use_cache = False
model.config.pretraining_tp = 1

tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path = "aboonaji/llama2finetune-v2", trust_remote_code = True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

training_arguments = TrainingArguments(
    output_dir = "./results",
    per_device_train_batch_size = 4,
    # per_device_eval_batch_size=2,
    max_steps = 100
    )

trainer = SFTTrainer(
    model=model,
    args = training_arguments,
    tokenizer = tokenizer,
    train_dataset = load_dataset(path="vivekdugale/falcon7B_filtered_dataset_amod_helios_450", split='train'),
    peft_config = LoraConfig(task_type="CAUSAL_LM", r=64, lora_alpha=16, lora_dropout=0.1),
    dataset_text_field = "text"
    )

trainer.train()

text_gen_pipeline = pipeline(task = "text-generation", model = model, tokenizer = tokenizer, max_length = 100)

!pip install pyngrok
# port_no=4000
# import requests
# from flask import Flask
# import requests
# from pyngrok import ngrok

# app = Flask(__name__)
# ngrok.set_auth_token("auth_token")
# public_url=ngrok.connect(port_no)

# print("NGROK - ", public_url)
import psutil

def terminate_processes_on_port(port):
    for proc in psutil.process_iter(['pid', 'name', 'connections']):
        for conn in proc.info.get('connections', []):
            if conn.laddr and conn.laddr.port == port:
                print(f"Terminating process {proc.pid} - {proc.name()}")
                proc.terminate()

if __name__ == "__main__":
    port_to_terminate = 6000
    terminate_processes_on_port(port_to_terminate)

from flask import Flask, jsonify, request
from pyngrok import ngrok
import threading

port = 6000

app = Flask(__name__)
ngrok.set_auth_token("auth_token")
public_url = ngrok.connect(port).public_url
print("NGROK ", public_url)
app.secret_key='password'

@app.route('/', methods=['POST'])
def llm():
    prompt = request.json['prompt']
    ans = text_gen_pipeline(f"[INST] {prompt} [/INST]")
    return jsonify({'text': ans[0]['generated_text']})

threading.Thread(target=app.run(port = port), kwargs={"use_reloader": False}).start()

from flask import Flask, jsonify, request
from pyngrok import ngrok
import requests

ngrok.set_auth_token("auth_token_goes_here")
public_url = ngrok.connect(4000).public_url
print("NGROK - ", public_url)
import requests
from flask import Flask

app=Flask(__name__)
# Assuming the Flask app is running on localhost and port 5000
url = "http://localhost:5000/chatbot"

@app.route("/")
def route():
# Send a GET request to the Flask app
    response = requests.get(url)
    print("HELLO")
    # return {"In":"Local host 5000"}

# Check if the request was successful (status code 200)
    if response.status_code == 200:
    # Extract the JSON data from the response
        data = response.json()
        print(data)
        return {"index":"Successful"}
    else:
        print("SOMETHING ELSE")
        return{"Error": response.status_code}

app.run(debug=True)

from google.colab import drive
drive.mount('/content/drive')
model.save("/content/drive/MyDrive/llm.h5")
